{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Summary: Mastering the game of Go with deep neural networks and tree search\n",
    "\n",
    "**Keywords:** Game-playing, Go, AlphaGo, DeepMind, MiniMax, Alpha-Beta Pruning, Monte-Carlo Tree Search, Convolutional Neural Networks, Reinforcement Learning.\n",
    "**Blog post:** [AlphaGo: using machine learning to master the ancient game of Go](https://blog.google/topics/machine-learning/alphago-machine-learning-game-go/)\n",
    "**IDs:** `doi:10.1038/nature16961`\n",
    "\n",
    "## Goals and introduced techniques\n",
    "\n",
    "A problem in solving Go games is the enormous search space that makes a complete search using standard algorithms such as alpha-beta pruned MiniMax virtually impossible. A game of Go consists of about `150` turns with an average of `250` possible moves per turn, resulting in about `5e+359` possible combinations. The number of atoms in the universe, on the other hand, is estimated to be around `1e+78` to `1e+82`.\n",
    "\n",
    "The paper describes the usage of convolutional neural networks to enhance the search and introduces the concepts of *value networks* for position evaluation and *policy networks* for move selection. These networks are trained in both a supervised fashion using human-played games, as well as against itself using reinforcement learning. By selecting previous states of the network at random as the opponent, the training algorithms is prevented from overfitting onto the current game state (e.g. \"memoizing\" the board).\n",
    "In addition, Monte Carlo simulations are used in conjunction with the value and policy networks to improve the search.\n",
    "\n",
    "### Techniques\n",
    "\n",
    "To shortcut search, not all board states `s` are fully explored in order to obtain their real value `v(s)`, but rather estimated using a value heuristic `v*(s)`, such that `v(s) ≈ v*(s)`. Additionally, a *Monte Carlo Tree Search* (MCTS) method randomly selects board states to be fully explored (\"rolled out\") until endgame without considering branching in order to obtain a probability distribution over the outcome of each move.\n",
    "\n",
    "Convolutional neural networks are used over the board state (as a `19x19` cell image) to reduce search depth and breadth over each move (in the value network), as well as for move selection (in the policy network):\n",
    "\n",
    "* The value network `v(θ)` is trained by regression (rather than classification) to predict whether the current player wins.\n",
    "* The 13-layer policy network `p(σ)` is trained directly from expert human moves using supervised learning (SL). This is reported to result in very high quality gradients for training.\n",
    "* A smaller \"fast policy\" network `p(π)` is trained to rapidly sample actions during MCTS rollouts.\n",
    "- A policy network `p(ρ)` is then trained using reinforcement learning (RL) in order to adjust the `p(σ)` network on the goal of winning the game.\n",
    "\n",
    "![Policy and value networks](alphago/alphago.jpg)\n",
    "\n",
    "AlphaGo then combines these networks and the MCTS algorithm.\n",
    "\n",
    "## Results\n",
    "\n",
    "AlphaGo achieves a reported `99.8%` winning rate against other Go programs and defeated the European Go champion by `5:0`.\n",
    "\n",
    "The SL policy network `p(σ)` achieved a prediction accuracy of `57%` of move predictions on a held-out dataset of expert player moves. When only raw board positions (and their history) were used as inputs, accuracy dropped slightly to `55.7%` (thus still outperforming the previously reported best accuracy of `44.4%`). It was noted that even small improvements in accuracy vastly improved playing strength.\n",
    "Larger nets are reported to perform better, but simultaneously take longer to evaluate.\n",
    "The fast policy network `p(π)` achieved an accuracy of `24.2%` while taking only `2µs` to evaluate (compared to the `3ms` taken by the SL/RL policy networks).\n",
    "\n",
    "![Policy network depth vs. win rate](alphago/alphago-policy-depth.jpg)\n",
    "\n",
    "The RL policy network achieved a `80%` win rate against the SL policy network directly and a `85%` win rate against the [Pachi](http://pachi.or.cz/) Go software using no search at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
